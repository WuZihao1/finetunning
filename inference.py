from transformers import pipeline, AutoModelForCausalLM
from model_setup import setup_model_and_tokenizer
import torch
import config
from peft import PeftModel


def generate_marketing_copy():
    """
    ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ç”Ÿæˆè¥é”€æ–‡æ¡ˆ
    ç›®çš„ï¼š
    1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    2. æ¥å—ç”¨æˆ·è¾“å…¥/ç¤ºä¾‹è¾“å…¥
    3. ç”Ÿæˆç¬¦åˆè¦æ±‚çš„è¥é”€æ–‡æ¡ˆ
    """
    cfg = config.TrainingConfig

    # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = setup_model_and_tokenizer()
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœä½¿ç”¨LoRAä¸”åªä¿å­˜äº†é€‚é…å™¨æƒé‡
    if cfg.USE_LORA:
        # åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆåŸå§‹é¢„è®­ç»ƒæ¨¡å‹ï¼‰
        base_model = AutoModelForCausalLM.from_pretrained(
            cfg.MODEL_NAME,
            device_map="auto",
            torch_dtype=torch.bfloat16
        )
        # å°†LoRAé€‚é…å™¨åŠ è½½åˆ°åŸºç¡€æ¨¡å‹ä¸Š
        model = PeftModel.from_pretrained(
            base_model,
            cfg.OUTPUT_DIR + "/final_model/lora_adapter"
        )
        model = model.to(device)
    else:
        # ç›´æ¥åŠ è½½å®Œæ•´æ¨¡å‹
        model = model.to(device)

    # 2. åˆ›å»ºæ–‡æœ¬ç”ŸæˆPipeline
    text_generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=device if device != "cpu" else -1,
    )

    # 3. æç¤ºæ¨¡æ¿ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    def create_prompt(instruction, input_text=None):
        """æ„é€ ä¸è®­ç»ƒæ ¼å¼ä¸€è‡´çš„æç¤º"""
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å¸‚åœºè¥é”€æ–‡æ¡ˆåŠ©æ‰‹ã€‚è¯·æ ¹æ®è¦æ±‚ç”Ÿæˆæ–‡æ¡ˆã€‚"

        if input_text:
            return f"[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n\n{instruction}\n{input_text} [/INST] "
        return f"[INST] <<SYS>>\n{system_prompt}\n<</SYS>>\n\n{instruction} [/INST] "

    # 4. ç¤ºä¾‹è¾“å…¥ï¼ˆç”Ÿäº§ç¯å¢ƒä¸­å¯æ›¿æ¢ä¸ºAPIæ¥æ”¶çš„è¾“å…¥ï¼‰
    examples = [
        {
            "instruction": "ä¸ºæƒ…äººèŠ‚ä¿ƒé”€æ´»åŠ¨å†™é‚®ä»¶æ ‡é¢˜å’Œç®€çŸ­æ­£æ–‡",
            "input": "äº§å“ï¼šæ‰‹å·¥å®šåˆ¶å·§å…‹åŠ›ç¤¼ç›’ï¼›æŠ˜æ‰£ï¼šç¬¬äºŒä»¶åŠä»·ï¼›é™æ—¶ï¼š2.10-2.14"
        },
        {
            "instruction": "ä¸ºå¤å­£é˜²æ™’éœœåˆ›å»ºå¾®åšæ¨å¹¿æ–‡æ¡ˆ",
            "input": "å“ç‰Œï¼šSunGuardï¼›SPF50+ï¼›ç‰¹ç‚¹ï¼šæ¸…çˆ½ä¸ç²˜è…»ã€é˜²æ°´é˜²æ±—"
        },
        {
            "instruction": "ä¸ºæ–°æ¬¾æ™ºèƒ½æ‰‹æœºè®¾è®¡Instagramå¹¿å‘Šæ–‡æ¡ˆ",
            "input": "å“ç‰Œï¼šNovaTech X6ï¼›äº®ç‚¹ï¼š200MPç›¸æœºã€å…¨å¤©ç»­èˆªã€æŠ˜å å±"
        }
    ]

    # 5. ç”Ÿæˆè¥é”€æ–‡æ¡ˆ
    print("ğŸŒŸ å¾®è°ƒæ¨¡å‹è¥é”€æ–‡æ¡ˆç”Ÿæˆæ¼”ç¤º ğŸŒŸ\n")
    for i, example in enumerate(examples, 1):
        prompt = create_prompt(example["instruction"], example.get("input"))
        print(f"### ç¤ºä¾‹ #{i}:")
        print(f"æŒ‡ä»¤: {example['instruction']}")
        if "input" in example:
            print(f"é™„åŠ ä¿¡æ¯: {example['input']}")

        # ç”Ÿæˆæ–‡æ¡ˆ
        results = text_generator(
            prompt,
            **cfg.GENERATION_CONFIG
        )

        # ä»å®Œæ•´å“åº”ä¸­æå–ç”Ÿæˆçš„æ–‡æ¡ˆ
        generated_text = results[0]['generated_text']
        # æå–æ¨¡å‹ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆä»[/INST]æ ‡è®°ä¹‹åï¼‰
        if "[/INST]" in generated_text:
            generated_text = generated_text.split("[/INST]")[1].strip()

        print("\nç”Ÿæˆæ–‡æ¡ˆ:")
        print(generated_text)
        print("\n" + "-" * 80 + "\n")


if __name__ == "__main__":
    generate_marketing_copy()