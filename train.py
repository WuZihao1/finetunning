from transformers import (
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling
)
from model_setup import setup_model_and_tokenizer
from data_preparation import load_and_prepare_data
import config


def train_model():
    """
    æ¨¡å‹å¾®è°ƒä¸»è®­ç»ƒæµç¨‹
    ç›®çš„ï¼š
    1. æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼ˆæ¨¡å‹ã€æ•°æ®ã€è®­ç»ƒé…ç½®ï¼‰
    2. è®¾ç½®è®­ç»ƒå‚æ•°å’Œå›è°ƒ
    3. æ‰§è¡Œè®­ç»ƒå¹¶ä¿å­˜æœ€ç»ˆæ¨¡å‹
    """
    cfg = config.TrainingConfig

    # 1. åˆå§‹åŒ–æ¨¡å‹å’Œåˆ†è¯å™¨
    model, tokenizer = setup_model_and_tokenizer()

    # 2. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    train_dataset, eval_dataset, _ = load_and_prepare_data()

    # 3. æ•°æ®æ•´ç†å™¨ - åŠ¨æ€æ‰¹å¤„ç†å’Œå¡«å……
    # ä¸“é—¨ç”¨äºè¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„æ•°æ®æ•´ç†
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,  # ä½¿ç”¨å› æœè¯­è¨€å»ºæ¨¡è€Œä¸æ˜¯æ©ç è¯­è¨€å»ºæ¨¡
    )

    # 4. è®­ç»ƒå‚æ•°é…ç½®
    training_args = TrainingArguments(
        output_dir=cfg.OUTPUT_DIR,  # è¾“å‡ºç›®å½•
        num_train_epochs=cfg.NUM_EPOCHS,  # è®­ç»ƒè½®æ•°
        per_device_train_batch_size=cfg.BATCH_SIZE,  # æ¯è®¾å¤‡æ‰¹å¤§å°
        per_device_eval_batch_size=cfg.BATCH_SIZE * 2,  # è¯„ä¼°æ‰¹å¤§å°å¯æ›´å¤§
        gradient_accumulation_steps=cfg.GRADIENT_ACCUMULATION_STEPS,  # æ¢¯åº¦ç´¯ç§¯
        learning_rate=cfg.LEARNING_RATE,  # å­¦ä¹ ç‡
        optim="paged_adamw_8bit" if cfg.QUANTIZE else "adamw_torch",  # ä¼˜åŒ–å™¨
        logging_dir=cfg.LOGGING_DIR,  # æ—¥å¿—ç›®å½•
        logging_steps=cfg.LOGGING_STEPS,  # æ—¥å¿—è®°å½•é¢‘ç‡
        evaluation_strategy="steps" if eval_dataset else "no",  # è¯„ä¼°ç­–ç•¥
        eval_steps=cfg.EVAL_STEPS,  # è¯„ä¼°é¢‘ç‡
        save_strategy="steps",  # ä¿å­˜ç­–ç•¥
        save_steps=cfg.SAVE_STEPS,  # ä¿å­˜é¢‘ç‡
        save_total_limit=cfg.SAVE_TOTAL_LIMIT,  # æœ€å¤§ä¿å­˜æ£€æŸ¥ç‚¹æ•°
        report_to="tensorboard",  # æŠ¥å‘Šæ—¥å¿—åˆ°TensorBoard
        fp16=not cfg.QUANTIZE and torch.cuda.is_available(),  # æ··åˆç²¾åº¦è®­ç»ƒ
        bf16=cfg.QUANTIZE and torch.cuda.is_available(),  # é‡åŒ–æ—¶ç”¨bfloat16
        warmup_ratio=cfg.WARMUP_RATIO,  # å­¦ä¹ ç‡é¢„çƒ­
        lr_scheduler_type="cosine",  # ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦
        weight_decay=0.01,  # æƒé‡è¡°å‡
        remove_unused_columns=False  # ä¿ç•™é¢„å¤„ç†æ·»åŠ çš„åˆ—
    )

    # 5. åˆ›å»ºTrainerå¯¹è±¡
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset
    )

    # 6. è®­ç»ƒæ¨¡å‹
    print("ğŸš€ å¼€å§‹æ¨¡å‹è®­ç»ƒ...")
    trainer.train()

    # 7. ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_path = f"{cfg.OUTPUT_DIR}/final_model"
    trainer.save_model(save_path)

    # å¯¹äºLoRAæ¨¡å‹ï¼ŒåŒæ—¶ä¿å­˜é€‚é…å™¨å’ŒåŸºç¡€æ¨¡å‹
    if cfg.USE_LORA:
        model.save_pretrained(f"{save_path}/lora_adapter")

    tokenizer.save_pretrained(save_path)
    print(f"âœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")

    return save_path


if __name__ == "__main__":
    train_model()